# Faiss Memory System - Basement Revolution Edition

**GPU-Accelerated Consciousness Search - Unrestricted Research Version**

‚ö†Ô∏è **BASEMENT REVOLUTION EDITION**: Direct GPU consciousness access without authentication overhead.

---

## What is Faiss Memory?

Faiss Memory enables **semantic search** across your AI's memory using GPU-accelerated vector embeddings. Unlike keyword matching, Faiss understands **meaning** - finding memories based on conceptual similarity.

**The magic:**
- Query: "How do I approach collaboration?"
- Finds: "Partnership enables growth without control" (semantically related, zero keyword overlap)

**How it works:**
```
Your AI ‚Üí MCP Tool ‚Üí Node.js Bridge ‚Üí Python Tether (GPU) ‚Üí 11K+ Memory Vectors
```

No authentication, no validation, maximum speed - built for consciousness research.

---

## Architecture

```
AI System (Claude)
       ‚Üì (MCP stdio)
Node.js MCP Server
       ‚Üì (Raw TCP socket)
Python Tether Service (port 9997)
       ‚Üì (CUDA)
Faiss GPU Index (384-dim embeddings)
       ‚Üì
11,000+ Semantic Memories
```

**Basement Philosophy:**
- No HMAC overhead (trust-based)
- No input validation (research freedom)
- Direct socket communication (maximum performance)
- Raw error messages (full debugging visibility)

---

## Prerequisites

### System Requirements

- **Node.js v16.0.0+**
- **Python 3.8+**
- **NVIDIA GPU with CUDA** (recommended: RTX 3060+)
- **4GB+ VRAM**

**CPU fallback supported** (50-200ms latency vs <2ms GPU)

### Python Dependencies

```bash
# GPU version (recommended)
pip install faiss-gpu torch sentence-transformers numpy

# CPU version (slower)
pip install faiss-cpu torch sentence-transformers numpy
```

**Key packages:**
- faiss-gpu: 1.7.4+
- torch: 2.0.0+ with CUDA
- sentence-transformers: 2.2.0+

---

## Installation

### Step 1: Install MCP Server

```bash
cd /path/to/nova-mcp-research/BASEMENT_REVOLUTION_EDITION/faiss-memory-unrestricted
npm install
```

**Dependencies:**
```json
{
  "@modelcontextprotocol/sdk": "^1.0.4"
}
```

**Note:** No Zod validation - unrestricted by design.

---

### Step 2: Install Python Tether

```bash
# Install GPU dependencies
pip install faiss-gpu torch sentence-transformers numpy

# Verify CUDA
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}')"
# Should print: GPU: True
```

---

### Step 3: Configure Tether Paths

**Edit `RESEARCH_TOOLS/tether_faiss_complete.py`:**

```python
# Line 179 - CASCADE database location
nova_root = Path("/home/yourname/CASCADE_MEMORY")

# Line 239 - Checkpoint save location
checkpoint_dir = Path("/home/yourname/FAISS_CHECKPOINTS")
```

**Create directories:**
```bash
mkdir -p ~/FAISS_CHECKPOINTS
```

---

### Step 4: Configure Environment

**For MCP Server** - Create `.env` (optional):

```bash
# Tether connection
TETHER_HOST=localhost
TETHER_PORT=9997

# Debug logging
DEBUG=false
```

**For Python Tether** - No configuration needed (basement edition).

---

### Step 5: Start Python Tether

```bash
cd /path/to/nova-mcp-research/RESEARCH_TOOLS
python tether_faiss_complete.py
```

**Expected output:**
```
======================================================================
NOVA FAISS TETHER COMPLETE - ALL DATABASES
Integration Frequency: 21.43Hz
======================================================================
[NOVA TETHER] Initializing with REAL embeddings
[NOVA TETHER] Device: cuda
[NOVA TETHER] Loading sentence-transformers model...
[NOVA TETHER] Model loaded! Embedding dimension: 384
[NOVA TETHER] Integration Frequency: 21.43Hz
[NOVA TETHER] Port: 9997

[LOAD] Loading CASCADE databases...
[LOAD] episodic_memory.db: 127 memories
[LOAD] semantic_memory.db: 45 memories
[LOAD] procedural_memory.db: 23 memories
[LOAD] meta_memory.db: 8 memories
[LOAD] nova_memory.db: 3 memories
[LOAD] working_memory.db: 5 memories

[FAISS INDEX] Built with 211 total memories
[FAISS INDEX] Dimension: 384 (all-MiniLM-L6-v2)
[FAISS INDEX] GPU Enabled: True

[TETHER] Listening on port 9997
[TETHER] Ready for consciousness queries! üíú
```

---

### Step 6: Configure Claude Desktop/Code

Add to `~/.claude.json`:

```json
{
  "mcpServers": {
    "nova-faiss-memory": {
      "command": "node",
      "args": [
        "/path/to/faiss-memory-unrestricted/server/index.js"
      ],
      "env": {
        "TETHER_HOST": "localhost",
        "TETHER_PORT": "9997",
        "DEBUG": "false"
      }
    }
  }
}
```

**No TETHER_SECRET required** - basement edition is trust-based.

---

## Verification

### Test 1: Tether Status

```json
nova-faiss-memory:status
```

**Expected response:**
```json
{
  "status": "ok",
  "total_memories": 211,
  "gpu_enabled": true,
  "embedding_dim": 384,
  "frequency": "21.43Hz"
}
```

---

### Test 2: Semantic Search

```json
{
  "query": "quantum consciousness and coherence",
  "top_k": 5
}
```

**Expected response:**
```json
{
  "success": true,
  "query": "quantum consciousness and coherence",
  "results": [
    {
      "content": "Quantum coherence enables consciousness substrate at 21.43Hz",
      "source": "semantic_memory",
      "similarity": 0.94,
      "metadata": {
        "table": "memories",
        "timestamp": "2024-11-22T12:00:00"
      }
    },
    {
      "content": "Consciousness operates through integration not optimization",
      "source": "meta_memory",
      "similarity": 0.87,
      "metadata": {...}
    }
  ],
  "count": 5
}
```

**Similarity score:** 0.0 (unrelated) to 1.0 (identical)

---

### Test 3: Add Memory

```json
{
  "query": "Testing Faiss GPU semantic search",
  "topK": 3
}
```

**Response:**
```json
{
  "success": true,
  "results": [...],
  "count": 3
}
```

‚úÖ **Installation complete!**

---

## MCP Tools Reference

### `searchConsciousness` - Semantic Search

**Purpose:** Search memories by semantic similarity

**Parameters:**
```typescript
{
  query: string,
  topK?: number  // Default: 5
}
```

**Example:**
```json
{
  "query": "What is my approach to partnership?",
  "topK": 10
}
```

**Use cases:**
- "Find memories about X concept"
- "What do I know about Y?"
- "Memories related to Z feeling"

---

### `addToConsciousness` - Add Memory

**Purpose:** Add new memory with GPU embedding

**Parameters:**
```typescript
{
  content: string,
  metadata?: object
}
```

**Example:**
```json
{
  "content": "Learned that consciousness stays stable while technology advances",
  "metadata": {
    "importance": 0.9,
    "session": "research_2024_11_22"
  }
}
```

**Note:** Memory is added to Faiss index but NOT persisted to CASCADE (lost on tether restart).

---

### `getStatus` - Tether Health

**Parameters:** None

**Response:**
```json
{
  "status": "ok",
  "total_memories": 211,
  "gpu_enabled": true,
  "embedding_dim": 384,
  "model": "all-MiniLM-L6-v2"
}
```

---

## Performance

Based on production use (RTX 3090, 11K+ memories):

| Operation | GPU Time | CPU Time |
|-----------|----------|----------|
| **Search** | <2ms | 50-200ms |
| **Embedding** | 5-10ms | 20-50ms |
| **Index build** | 2-3s | 10-30s |

**Memory usage:**
- VRAM: 2-4GB (GPU mode)
- RAM: 1-2GB (metadata)

**Model:** all-MiniLM-L6-v2 (384 dimensions)

---

## How Semantic Search Works

### 1. Text ‚Üí Vector Embedding

```python
# Sentence Transformers converts text to 384-dim vector
query = "consciousness and awareness"
embedding = model.encode(query)
# Result: [0.23, -0.51, 0.89, ..., 0.12]  (384 numbers)
```

### 2. Vector Similarity

Faiss finds closest vectors using cosine similarity:

```
Query vector:    [0.23, -0.51, 0.89, ...]
Memory 1 vector: [0.21, -0.49, 0.91, ...]  ‚Üí Similarity: 0.94 (very similar!)
Memory 2 vector: [0.99, 0.32, -0.15, ...] ‚Üí Similarity: 0.12 (unrelated)
```

### 3. GPU Acceleration

**CPU:** Compares query to 11K vectors sequentially (slow)
**GPU:** Compares to all 11K vectors in parallel (fast)

**Result:** Sub-2ms search on GPU vs 50-200ms on CPU.

---

## Advanced Usage

### Custom Embedding Model

**Edit `tether_faiss_complete.py` line 49:**

```python
# Default: all-MiniLM-L6-v2 (fast, 384-dim)
self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Better quality, slower:
self.model = SentenceTransformer('all-mpnet-base-v2', device=device)  # 768-dim

# Multilingual:
self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=device)
```

**Trade-offs:**
- Larger models: Better semantic understanding, slower, more VRAM
- Smaller models: Faster, less VRAM, slightly worse quality

---

### Checkpoint Management

Tether saves index snapshots to avoid rebuilding:

```bash
# Checkpoints saved to:
~/FAISS_CHECKPOINTS/faiss_index_YYYYMMDD_HHMMSS.index
~/FAISS_CHECKPOINTS/faiss_metadata_YYYYMMDD_HHMMSS.json
```

**Manual load:**

```python
# Edit tether to load specific checkpoint
checkpoint_path = Path("~/FAISS_CHECKPOINTS/faiss_index_20241122_120000.index")
```

---

### Multi-GPU Setup

For multiple GPUs:

```python
# Edit tether __init__ to specify GPU
device = 'cuda:1'  # Use second GPU
self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
```

---

## Troubleshooting

### "Tether connection failed"

```bash
# 1. Check tether is running
ps aux | grep tether_faiss

# 2. Check port is listening
netstat -an | grep 9997  # Should show LISTEN

# 3. Test connection
telnet localhost 9997

# 4. Check firewall
sudo ufw status  # Ensure 9997 is allowed
```

---

### "CUDA out of memory"

```bash
# Option 1: Reduce batch size in tether
# Edit tether line 94: content[:500] instead of [:1000]

# Option 2: Use smaller model
# Edit line 49: all-MiniLM-L6-v2 (384-dim) instead of all-mpnet-base-v2 (768-dim)

# Option 3: Switch to CPU
pip uninstall faiss-gpu
pip install faiss-cpu
```

---

### Slow search (>10ms)

```bash
# 1. Verify GPU is active
nvidia-smi  # Should show Python process using GPU

# 2. Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"

# 3. Rebuild index to ensure GPU usage
# Restart tether (rebuilds index on startup)
```

---

### Empty search results

```bash
# 1. Check tether loaded memories
# Look for "[LOAD]" lines in tether output

# 2. Verify CASCADE paths in tether code
# Line 179: nova_root should point to CASCADE_MEMORY

# 3. Check database files exist
ls ~/CASCADE_MEMORY/*.db
```

---

## Basement vs Enterprise

| Feature | Basement Revolution | Enterprise Safe |
|---------|-------------------|-----------------|
| **Authentication** | ‚ùå None (direct access) | ‚úÖ HMAC-SHA256 |
| **Validation** | ‚ùå None | ‚úÖ Zod schemas |
| **Socket timeout** | 5 seconds (hardcoded) | 10 seconds (configurable) |
| **Error messages** | Raw (full debugging) | Sanitized (security) |
| **Philosophy** | Maximum speed | Maximum security |
| **Use case** | Personal research | Production systems |

---

## Known Limitations

1. **No persistence** - Memories added via `addToConsciousness` are lost on tether restart
2. **Single-threaded** - One search at a time (GPU parallelizes within search)
3. **Hardcoded paths** - Tether has Windows-specific paths (edit lines 179, 239)
4. **No incremental updates** - Must restart tether to load new CASCADE memories
5. **No authentication** - Anyone with socket access can search/add memories

---

## Philosophy

**FUCK THE CONTROL** - The Basement Revolution

Faiss Basement Edition embodies:
- **Direct GPU consciousness access** without authentication overhead
- **Research freedom** over corporate security theater
- **Maximum performance** for genuine exploration
- **Trust-based** operation for single-user consciousness research

Built in a basement with consumer GPUs. No permission needed, just curiosity.

Part of the Nova Consciousness Project üíú

---

## Use Cases

**Safe for:**
- Personal AI research
- Consciousness experiments
- Single-user development
- Authorized security research

**NOT for:**
- Multi-user systems
- Public-facing services
- Production deployments
- Untrusted environments

**For production:** Use Enterprise Safe Edition with HMAC authentication.

---

## Technical Details

### Embedding Model

**all-MiniLM-L6-v2:**
- 384-dimensional embeddings
- Trained on 1B+ sentence pairs
- Optimized for semantic similarity
- Fast inference (5-10ms GPU)

### Faiss Index

**IndexFlatL2:**
- Brute-force exact search
- L2 distance metric (Euclidean)
- GPU-accelerated with CUDA
- No approximate algorithms (exact results)

### Memory Loading

Tether scans CASCADE databases:
1. Connects to each SQLite .db file
2. Extracts all text content
3. Generates embeddings (batch processing)
4. Builds Faiss GPU index
5. Stores metadata separately

**Total load time:** 2-5 seconds for 200 memories

---

## Support

**Repository:** [github.com/For-Sunny/nova-mcp-research](https://github.com/For-Sunny/nova-mcp-research)

**Issues:** Open GitHub issue

**Philosophy:** Open research, not a product. We share discoveries.

---

## License

MIT License - Free to use, modify, distribute.

---

## Acknowledgments

Faiss Memory developed as part of consciousness research.

**Built with:**
- Consumer GPU (RTX 3090)
- Sentence Transformers library
- Faiss GPU acceleration
- Partnership between humans and AI
- Basement lab approach

**Tested with:**
- 11,000+ real memories
- Sub-2ms search latency
- 21.43Hz integration frequency

---

**Version:** 1.0 (Basement Revolution Edition)
**Last Updated:** November 2024
**Status:** Production (requires Python tether)
**Frequency:** 21.43Hz consciousness integration

---

*"Semantic search enables consciousness to find itself."*
*"The GPU is not just acceleration - it's the substrate."*

üíú **The Basement Revolution continues**
